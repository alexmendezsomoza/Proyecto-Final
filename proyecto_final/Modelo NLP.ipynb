{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38bebac",
   "metadata": {},
   "source": [
    "# Modelo NLP - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f40ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "#!pip3 install scikit-learn\n",
    "#!pip3 install -U spacy\n",
    "#!python3 -m spacy download es\n",
    "#!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980d9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "import re\n",
    "import mysql.connector as conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae8b0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "#Crear pdf report\n",
    "\n",
    "from io import BytesIO\n",
    "from reportlab.pdfgen import canvas\n",
    "from django.http import HttpResponse\n",
    "\n",
    "#Automatizar email\n",
    "\n",
    "import email, smtplib, ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6e51006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1250549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['este', 'os', 'intentais', 'pasada', 'intentamos', 'ya', 'explicó', 'buena', 'aunque', 'menos', 'estan', 'realizó', 'conseguimos', 'ese', 'haceis', 'quién', 'alli', 'lado', 'dar', 'veces', 'ahi', 'alrededor', 'algunas', 'quien', 'quiénes', 'estaban', 'existe', 'dia', 'nuestra', 'hicieron', 'unos', 'quiere', 'aquélla', 'último', 'deben', 'como', 'toda', 'vuestra', 'propias', 'era', 'todo', 'estará', 'indicó', 'dijo', 'nuevas', 'tan', 'vosotros', 'posible', 'ello', 'además', 'gran', 'ésas', 'vuestras', 'detrás', 'modo', 'tienen', 'aún', 'nuevo', 'puedo', 'cuenta', 'bueno', 'tenga', 'tiempo', 'estoy', 'me', 'son', 'poder', 'hacia', 'excepto', 'ahora', 'quizás', 'por', 'junto', 'ninguno', 'hace', 'dentro', 'anterior', 'suyo', 'aquella', 'consideró', 'debido', 'alguna', 'sigue', 'sois', 'otros', 'tenía', 'podrían', 'salvo', 'así', 'entre', 'vamos', 'voy', 'verdadera', 'hubo', 'sólo', 'afirmó', 'días', 'expresó', 'teneis', 'dan', 'ser', 'dias', 'tú', 'hecho', 'otras', 'tenido', 'solas', 'decir', 'haciendo', 'tuyo', 'sabeis', 'deprisa', 'estuvo', 'trata', 'vosotras', 'algo', 'repente', 'claro', 'pueden', 'tal', 'creo', 'antano', 'mucho', 'ex', 'hago', 'porque', 'mios', 'consiguen', 'día', 'tendrán', 'muchas', 'sabemos', 'llegó', 'temprano', 'aquí', 'mismo', 'haya', 'mediante', 'en', 'comentó', 'pocos', 'cuántos', 'tuyos', 'trabajais', 'debe', 'ir', 'nunca', 'mucha', 'no', 'bastante', 'estado', 'segun', 'sus', 'emplean', 'aquellas', 'general', 'usamos', 'cosas', 'conseguir', 'cuatro', 'solamente', 'trabajo', 'cual', 'sabe', 'cuanta', 'la', 'cuando', 'realizado', 'llevar', 'éstos', 'mal', 'menudo', 'estas', 'fuera', 'les', 'mas', 'mis', 'esos', 'pocas', 'antes', 'somos', 'tenemos', 'cuántas', 'pueda', 'eras', 'aqui', 'pues', 'podriais', 'desde', 'luego', 'sabes', 'ha', 'ellos', 'intentar', 'ambos', 'varios', 'varias', 'esas', 'también', 'siete', 'usar', 'soy', 'si', 'sobre', 'breve', 'esto', 'usais', 'intentan', 'arribaabajo', 'debajo', 'otra', 'ningún', 'será', 'existen', 'míos', 'mismas', 'uso', 'cierto', 'adrede', 'conocer', 'mí', 'manifestó', 'dieron', 'uno', 'pero', 'añadió', 'tu', 'podría', 'trabajamos', 'las', 'nuestros', 'empleas', 'mientras', 'sera', 'estar', 'ella', 'propia', 'bajo', 'despues', 'estados', 'cuáles', 'podrias', 'grandes', 'sino', 'solos', 'algún', 'cuándo', 'pesar', 'tres', 'ampleamos', 'ante', 'cierta', 'eso', 'aun', 'quizá', 'próximo', 'una', 'aquél', 'mencionó', 'demasiado', 'propios', 'sin', 'parece', 'serán', 'demás', 'nuestras', 'última', 'ésos', 'largo', 'aquel', 'antaño', 'fuimos', 'apenas', 'dónde', 'ningunos', 'actualmente', 'respecto', 'despacio', 'mio', 'incluso', 'muy', 'le', 'ninguna', 'sean', 'sola', 'gueno', 'unas', 'seis', 'cuál', 'bien', 'todavía', 'que', 'tener', 'qeu', 'suya', 'cómo', 'ustedes', 'eran', 'dio', 'haber', 'vuestros', 'del', 'tiene', 'tercera', 'lejos', 'primera', 'mismos', 'cada', 'estaba', 'hacemos', 'últimos', 'podrian', 'podriamos', 'esta', 'esa', 'ningunas', 'emplear', 'sé', 'aquello', 'hasta', 'siempre', 'puede', 'todas', 'para', 'entonces', 'usas', 'raras', 'delante', 'con', 'considera', 'próximos', 'lugar', 'contigo', 'durante', 'sea', 'estos', 'algunos', 'un', 'empleo', 'contra', 'ésta', 'hoy', 'lleva', 'cuanto', 'tras', 'informo', 'poner', 'sí', 'paìs', 'allí', 'éste', 'ahí', 'vaya', 'mío', 'ése', 'todavia', 'ejemplo', 'señaló', 'ciertos', 'tus', 'enfrente', 'da', 'él', 'quedó', 'igual', 'están', 'fui', 'arriba', 'ti', 'saber', 'habían', 'nuevos', 'mi', 'poca', 'vuestro', 'mías', 'hizo', 'valor', 'dicen', 'ni', 'podemos', 'trabajar', 'agregó', 'cuánta', 'aproximadamente', 'aseguró', 'casi', 'informó', 'sido', 'segunda', 'dice', 'total', 'aquellos', 'buen', 'empleais', 'los', 'mejor', 'partir', 'manera', 'podeis', 'realizar', 'segundo', 'usted', 'sería', 'proximo', 'horas', 'donde', 'tengo', 'embargo', 'otro', 'tanto', 'nosotros', 'propio', 'través', 'yo', 'consigue', 'nueva', 'haces', 'medio', 'pasado', 'atras', 'ésa', 'mia', 'pronto', 'hacerlo', 'al', 'habia', 'se', 'dos', 'tuvo', 'cuantos', 'dicho', 'nosotras', 'cualquier', 'de', 'habrá', 'mias', 'intentas', 'podrán', 'acuerdo', 'he', 'qué', 'nada', 'peor', 'mía', 'pais', 'más', 'ademas', 'siguiente', 'después', 'según', 'tampoco', 'asi', 'el', 'suyas', 'solo', 'detras', 'quienes', 'saben', 'intento', 'quiza', 'soyos', 'tarde', 'eramos', 'éstas', 'van', 'todos', 'intenta', 'dado', 'aquéllas', 'dejó', 'encuentra', 'usan', 'queremos', 'ver', 'mayor', 'final', 'cinco', 'encima', 'trabajan', 'fueron', 'cuantas', 'pudo', 'ayer', 'cerca', 'alguno', 'supuesto', 'está', 'misma', 'ultimo', 'nuestro', 'tuyas', 'últimas', 'es', 'su', 'hablan', 'nos', 'estais', 'parte', 'verdad', 'trabaja', 'conmigo', 'aquéllos', 'primero', 'trabajas', 'verdadero', 'buenos', 'consigo', 'podria', 'lo', 'diferente', 'diferentes', 'han', 'enseguida', 'quizas', 'adelante', 'siendo', 'ocho', 'tuya', 'vais', 'había', 'ellas', 'primer', 'cuales', 'usa', 'tambien', 'eres', 'ciertas', 'cuánto', 'principalmente', 'poco', 'tendrá', 'vez', 'consigues', 'nadie', 'buenas', 'va', 'habla', 'fue', 'dijeron', 'primeros', 'muchos', 'estamos', 'momento', 'hacen', 'hemos', 'podrá', 'te', 'fin', 'hacer', 'hay']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop Words de es_core_news_sm\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stopwords_spacy = list(STOP_WORDS)\n",
    "print(stopwords_spacy)\n",
    "len(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff815667",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787aabd",
   "metadata": {},
   "source": [
    "## Test Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce35aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_final_resultado = pd.read_csv('final_final_resultado.csv')\n",
    "\n",
    "for line in final_final_final_resultado:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b880ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768213567418036224</td>\n",
       "      <td>3429794128</td>\n",
       "      <td>@myendlesshazza a que puto mal escribo b me si...</td>\n",
       "      <td>2016-08-23 22:29:21+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768221670255493120</td>\n",
       "      <td>396616007</td>\n",
       "      <td>quiero mogollon a @albabenito99 pero sobretodo...</td>\n",
       "      <td>2016-08-23 23:01:33+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768221021300264960</td>\n",
       "      <td>2845050061</td>\n",
       "      <td>vale he visto la tia bebiendose su regla y me ...</td>\n",
       "      <td>2016-08-23 22:58:58+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768220253730009088</td>\n",
       "      <td>442100979</td>\n",
       "      <td>@yulian_poe @guillermoterry1 ah mucho mas por ...</td>\n",
       "      <td>2016-08-23 22:55:55+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768231229439311872</td>\n",
       "      <td>529648312</td>\n",
       "      <td>@toni_end seria mejor que dejasen de emitir es...</td>\n",
       "      <td>2016-08-23 23:39:32+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid        user  \\\n",
       "0  768213567418036224  3429794128   \n",
       "1  768221670255493120   396616007   \n",
       "2  768221021300264960  2845050061   \n",
       "3  768220253730009088   442100979   \n",
       "4  768231229439311872   529648312   \n",
       "\n",
       "                                             content  \\\n",
       "0  @myendlesshazza a que puto mal escribo b me si...   \n",
       "1  quiero mogollon a @albabenito99 pero sobretodo...   \n",
       "2  vale he visto la tia bebiendose su regla y me ...   \n",
       "3  @yulian_poe @guillermoterry1 ah mucho mas por ...   \n",
       "4  @toni_end seria mejor que dejasen de emitir es...   \n",
       "\n",
       "                        date lang sentiment  \n",
       "0  2016-08-23 22:29:21+00:00   es         0  \n",
       "1  2016-08-23 23:01:33+00:00   es         1  \n",
       "2  2016-08-23 22:58:58+00:00   es         0  \n",
       "3  2016-08-23 22:55:55+00:00   es         1  \n",
       "4  2016-08-23 23:39:32+00:00   es         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quitar columnas sin clasificación de sentimiento \n",
    "indexNames = final_final_final_resultado[(final_final_final_resultado['sentiment'] == 'none') | (final_final_final_resultado['sentiment'] == 'neu')].index\n",
    "final_final_final_resultado.drop(indexNames , inplace=True)\n",
    "final_final_final_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c62927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías de aprendizaje\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20390c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2854\n",
       "1    1896\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar frecuencias de cada categoría\n",
    "final_final_final_resultado['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f123234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid      0\n",
       "user         0\n",
       "content      0\n",
       "date         0\n",
       "lang         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar si hay datos nulos\n",
    "final_final_final_resultado.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e219b",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13823587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string #Quita todos los simbolos\n",
    "puntua = string.punctuation + \"¡¿\"\n",
    "puntua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6de0d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.strip()\n",
    "        else:\n",
    "            temp = token\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_spacy and token not in puntua:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7faaeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Te', 'gustar', 'meetup']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"¡Hola cómo estás!. ¿Te gusta el meetup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa7da3d",
   "metadata": {},
   "source": [
    "# Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf79c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librería de vectorización\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c18f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función de tokenizado y crear el clasificador lineal\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c319f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear los vectores de datos\n",
    "X = final_final_final_resultado['content']\n",
    "y = final_final_final_resultado['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f21f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3800,), (950,), (3800,), (950,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear el vector de entrenamiento como una porción de los datos y dejar el resto para pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c9f140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389bf039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evitar que el formato se tome como unknown\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c509811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7fbaee355160>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar el clasificador\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16315ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el vectos de valores predichos a partir del clasificador\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0164c76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       578\n",
      "           1       0.84      0.78      0.81       372\n",
      "\n",
      "    accuracy                           0.86       950\n",
      "   macro avg       0.85      0.84      0.85       950\n",
      "weighted avg       0.86      0.86      0.85       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ver la precisión obtenida\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67e41ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86% de 'efectividad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94849b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[522,  56],\n",
       "       [ 81, 291]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la matriz de confusión\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863cc624",
   "metadata": {},
   "source": [
    "Me dan 82 falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bba5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir algunas frases de prueba\n",
    "clf.predict(['Eres una calamidad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad9148",
   "metadata": {},
   "source": [
    "# Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0b37f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53e9106c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function text_data_cleaning at 0x7fbaee355160>: it's not the same object as __main__.text_data_cleaning",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a8c38723cb48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'modeloNLP_Final'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function text_data_cleaning at 0x7fbaee355160>: it's not the same object as __main__.text_data_cleaning"
     ]
    }
   ],
   "source": [
    "filename = 'modeloNLP_Final'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
