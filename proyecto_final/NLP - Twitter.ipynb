{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38bebac",
   "metadata": {},
   "source": [
    "# Modelo NLP - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f40ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "#!pip3 install scikit-learn\n",
    "#!pip3 install -U spacy\n",
    "#!python3 -m spacy download es\n",
    "#!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c26f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980d9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ccafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b198602",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ed9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hay un gato en la casa. Hay una granja en la nevera. Esta es una tercera frase'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90a03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay\n",
      "un\n",
      "gato\n",
      "en\n",
      "la\n",
      "casa\n",
      ".\n",
      "Hay\n",
      "una\n",
      "granja\n",
      "en\n",
      "la\n",
      "nevera\n",
      ".\n",
      "Esta\n",
      "es\n",
      "una\n",
      "tercera\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc: #ver los tokens/elemento de texto que encuentra la libreria\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a5e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orac = nlp.create_pipe('sentencizer') # 1ª Tuberia/Fase procesamiento. Esto separa la oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0454801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(orac, before='parser') # Se añade al pipe e identifica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1d4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc = nlp (text) #Volvemos a pasar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed71b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay un gato en la casa.\n",
      "Hay una granja en la nevera.\n",
      "Esta es una tercera frase\n"
     ]
    }
   ],
   "source": [
    "for orac in doc.sents: #Ahora, separa en frase\n",
    "    print(orac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02aeb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dice', 'trabajas', 'usais', 'durante', 'mismas', 'propia', 'podria', 'nunca', 'siempre', 'pueda', 'intenta', 'dos', 'cada', 'empleas', 'algunas', 'cosas', 'contigo', 'está', 'qeu', 'pues', 'tus', 'solo', 'aun', 'cuanto', 'usan', 'últimos', 'nuevos', 'nada', 'fuera', 'medio', 'dado', 'también', 'verdadera', 'demasiado', 'ninguna', 'proximo', 'próximo', 'estará', 'en', 'de', 'aseguró', 'pesar', 'ningunas', 'decir', 'próximos', 'nuestro', 'ir', 'lo', 'tengo', 'hay', 'día', 'sobre', 'encuentra', 'buenos', 'tendrán', 'no', 'existen', 'te', 'sabemos', 'pudo', 'ni', 'son', 'es', 'ademas', 'cierta', 'propio', 'otro', 'trata', 'sé', 'poca', 'dar', 'fuimos', 'con', 'última', 'vuestros', 'atras', 'quién', 'breve', 'hoy', 'segun', 'llevar', 'anterior', 'cual', 'poder', 'queremos', 'estais', 'suyas', 'yo', 'aquellas', 'haces', 'mí', 'somos', 'para', 'tuyos', 'total', 'sería', 'estas', 'deben', 'consideró', 'soyos', 'raras', 'todavía', 'buen', 'dentro', 'trabajan', 'explicó', 'nosotras', 'mías', 'muchas', 'cerca', 'os', 'van', 'los', 'qué', 'trabajais', 'asi', 'bien', 'éstas', 'tenga', 'tarde', 'debido', 'nuevas', 'intentar', 'quiénes', 'vosotras', 'hacer', 'quien', 'algo', 'debajo', 'ello', 'propios', 'vamos', 'buenas', 'haceis', 'quiza', 'arribaabajo', 'señaló', 'algunos', 'sea', 'hacia', 'aquélla', 'primer', 'consigue', 'nuestras', 'intentan', 'respecto', 'vuestra', 'haciendo', 'según', 'pronto', 'haber', 'mío', 'tiene', 'han', 'bajo', 'ninguno', 'despues', 'dia', 'aproximadamente', 'días', 'hemos', 'dónde', 'míos', 'hicieron', 'mismo', 'estamos', 'estado', 'estar', 'unas', 'ahí', 'va', 'donde', 'manera', 'tiempo', 'vuestro', 'hago', 'saben', 'delante', 'solos', 'tenemos', 'usar', 'menos', 'pocos', 'ha', 'gueno', 'me', 'pocas', 'peor', 'lugar', 'hubo', 'vuestras', 'nuestros', 'todo', 'eramos', 'cinco', 'cuántas', 'entonces', 'momento', 'mucha', 'tener', 'cualquier', 'enseguida', 'suya', 'cierto', 'fue', 'considera', 'si', 'ningunos', 'todas', 'mios', 'encima', 'segunda', 'podrian', 'vaya', 'consigues', 'diferente', 'esas', 'ultimo', 'ex', 'usas', 'realizar', 'del', 'estoy', 'hacerlo', 'nueva', 'tú', 'informo', 'trabajo', 'mientras', 'hizo', 'últimas', 'dias', 'siete', 'un', 'solas', 'tuyas', 'estan', 'otras', 'dijeron', 'cuatro', 'quienes', 'añadió', 'tenido', 'alrededor', 'tuyo', 'dan', 'parece', 'sabeis', 'sido', 'ti', 'eran', 'puede', 'allí', 'fui', 'contra', 'dicho', 'poner', 'toda', 'detrás', 'horas', 'vais', 'nuestra', 'quizas', 'serán', 'eso', 'suyo', 'aqui', 'antes', 'dejó', 'tendrá', 'tanto', 'usamos', 'ese', 'consigo', 'agregó', 'puedo', 'mias', 'lejos', 'así', 'intentais', 'partir', 'intento', 'él', 'quizás', 'teneis', 'conmigo', 'menudo', 'pueden', 'podemos', 'informó', 'ayer', 'creo', 'hasta', 'mismos', 'actualmente', 'modo', 'solamente', 'pero', 'afirmó', 'mas', 'bueno', 'paìs', 'cómo', 'apenas', 'tercera', 'cuales', 'además', 'quiere', 'sera', 'estaban', 'segundo', 'varias', 'ocho', 'ésas', 'general', 'cuando', 'esa', 'habia', 'le', 'estaba', 'ambos', 'algún', 'ante', 'éste', 'cuándo', 'aquello', 'mis', 'nadie', 'excepto', 'sigue', 'final', 'habían', 'desde', 'hablan', 'siguiente', 'gran', 'sí', 'uso', 'aquél', 'esos', 'conseguimos', 'casi', 'debe', 'vosotros', 'despacio', 'pais', 'les', 'sino', 'bastante', 'habla', 'fueron', 'estos', 'quedó', 'alguno', 'después', 'primeros', 'saber', 'podrias', 'trabajar', 'cuáles', 'repente', 'igual', 'cuenta', 'da', 'hecho', 'arriba', 'entre', 'dijo', 'lleva', 'antaño', 'realizado', 'sois', 'emplear', 'dieron', 'aquí', 'comentó', 'tambien', 'el', 'al', 'hace', 'ésa', 'están', 'fin', 'tu', 'ya', 'empleais', 'adelante', 'hacen', 'intentamos', 'ver', 'embargo', 'una', 'podriais', 'éstos', 'incluso', 'salvo', 'ellos', 'mucho', 'las', 'este', 'tienen', 'ningún', 'mayor', 'aquéllas', 'otra', 'ser', 'vez', 'siendo', 'ella', 'sola', 'que', 'por', 'tan', 'aún', 'la', 'consiguen', 'tres', 'ésta', 'pasada', 'claro', 'sólo', 'era', 'uno', 'hacemos', 'tal', 'través', 'adrede', 'conseguir', 'aquéllos', 'quizá', 'ustedes', 'manifestó', 'haya', 'cuánta', 'unos', 'temprano', 'antano', 'alli', 'indicó', 'sus', 'podría', 'poco', 'su', 'aquel', 'cuánto', 'esto', 'existe', 'diferentes', 'dio', 'mia', 'aquellos', 'primera', 'ampleamos', 'verdad', 'voy', 'principalmente', 'sabe', 'sabes', 'empleo', 'propias', 'acuerdo', 'mejor', 'trabajamos', 'buena', 'esta', 'todavia', 'conocer', 'había', 'emplean', 'sean', 'primero', 'llegó', 'expresó', 'intentas', 'enfrente', 'supuesto', 'podeis', 'estados', 'usted', 'misma', 'tras', 'otros', 'ése', 'será', 'muy', 'he', 'largo', 'posible', 'detras', 'tuya', 'verdadero', 'mio', 'tuvo', 'deprisa', 'ésos', 'tampoco', 'estuvo', 'ahora', 'mi', 'porque', 'muchos', 'trabaja', 'más', 'podrán', 'demás', 'ellas', 'ejemplo', 'ciertas', 'pasado', 'cuanta', 'aunque', 'junto', 'ahi', 'todos', 'cuál', 'se', 'eres', 'tenía', 'habrá', 'ciertos', 'alguna', 'cuántos', 'usa', 'podrían', 'eras', 'nos', 'podriamos', 'seis', 'podrá', 'mal', 'luego', 'realizó', 'como', 'mencionó', 'mediante', 'parte', 'sin', 'nuevo', 'lado', 'varios', 'nosotros', 'dicen', 'cuantos', 'grandes', 'cuantas', 'mía', 'aquella', 'último', 'veces', 'valor', 'soy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a eliminar las stop words\n",
    "\n",
    "stopwords_spacy = list(STOP_WORDS) #Checkar cuales son\n",
    "print(stopwords_spacy)\n",
    "len(stopwords_spacy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae93a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meter?{'todo', 'seáis', 'estada', 'estoy', 'se', 'habrás', 'hubierais', 'hubiésemos', 'esta', 'le', 'habidos', 'estuviesen', 'has', 'suyas', 'ha', 'tú', 'tuviese', 'como', 'eres', 'tenga', 'habíais', 'hubiese', 'sentida', 'fuerais', 'es', 'seré', 'mía', 'tengan', 'me', 'nosotros', 'quienes', 'mi', 'otra', 'estarán', 'hubieseis', 'cuando', 'fueran', 'seamos', 'vosotras', 'o', 'tuya', 'eso', 'fuésemos', 'en', 'siente', 'tendrían', 'hayan', 'el', 'tendrás', 'fui', 'tiene', 'habiendo', 'del', 'habríais', 'estar', 'vuestras', 'nuestro', 'serías', 'estos', 'hayáis', 'seremos', 'tenemos', 'ellas', 'tuvierais', 'teníamos', 'mías', 'una', 'nada', 'habré', 'tu', 'sus', 'hube', 'no', 'estuvieron', 'y', 'estabas', 'fueses', 'sean', 'tendríamos', 'vuestra', 'esto', 'habrán', 'tengo', 'poco', 'tenidas', 'estadas', 'tenida', 'estuve', 'otro', 'míos', 'tuyas', 'al', 'otras', 'muchos', 'estarían', 'estéis', 'sí', 'estuvieras', 'te', 'tuvieran', 'estaremos', 'algo', 'nosotras', 'habéis', 'seríais', 'suyo', 'hubiéramos', 'tuviera', 'habrá', 'estaréis', 'de', 'había', 'tendríais', 'habías', 'hubiesen', 'otros', 'hubiste', 'ti', 'sentido', 'fuimos', 'tuyos', 'porque', 'tengáis', 'qué', 'fuese', 'la', 'su', 'nuestras', 'están', 'tuvimos', 'estuviésemos', 'éramos', 'muy', 'mucho', 'vosotros', 'algunas', 'he', 'sentidos', 'estuviese', 'ella', 'fuiste', 'estarás', 'fuéramos', 'uno', 'tenían', 'fueseis', 'ni', 'suyos', 'estaría', 'nuestros', 'estén', 'os', 'tengamos', 'estarías', 'les', 'estábamos', 'también', 'e', 'tendrán', 'está', 'estés', 'tuviésemos', 'sentidas', 'antes', 'mí', 'estemos', 'estaríais', 'soy', 'estuvisteis', 'quien', 'tuvieses', 'estáis', 'que', 'fueron', 'esas', 'hubimos', 'tuvieras', 'entre', 'tuviste', 'unos', 'esté', 'serán', 'habida', 'estaban', 'hayamos', 'fue', 'todos', 'estaríamos', 'serás', 'ellos', 'estado', 'tenías', 'estando', 'estaba', 'estuviéramos', 'vuestros', 'habrías', 'hemos', 'fuisteis', 'tenidos', 'desde', 'esa', 'habría', 'tenéis', 'estuvierais', 'tendré', 'tendrías', 'hayas', 'hubieron', 'sintiendo', 'con', 'habréis', 'habido', 'tendremos', 'estad', 'teniendo', 'vuestro', 'será', 'ya', 'fuera', 'durante', 'hay', 'hubieses', 'han', 'más', 'estuviera', 'estuvieses', 'habíamos', 'son', 'para', 'tenido', 'tuvieron', 'sois', 'esos', 'por', 'estamos', 'sin', 'ese', 'nos', 'tus', 'tuvieseis', 'cual', 'hubisteis', 'habríamos', 'habrían', 'tenía', 'suya', 'eran', 'contra', 'mis', 'habremos', 'estuviste', 'a', 'hubieras', 'estuvieran', 'hubo', 'sentid', 'sobre', 'seas', 'somos', 'algunos', 'pero', 'eras', 'erais', 'tuve', 'hubiera', 'sería', 'hubieran', 'tened', 'tuyo', 'fuesen', 'tendrá', 'tienen', 'seréis', 'mío', 'sea', 'hasta', 'teníais', 'ante', 'tendría', 'nuestra', 'tuviesen', 'habían', 'habidas', 'este', 'un', 'serían', 'donde', 'lo', 'era', 'fueras', 'haya', 'estados', 'seríamos', 'tienes', 'estará', 'estabais', 'estuvieseis', 'estaré', 'las', 'los', 'tuviéramos', 'estás', 'tuvo', 'tendréis', 'tengas', 'estuvo', 'estas', 'él', 'tanto', 'estuvimos', 'tuvisteis', 'yo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7c3db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato\n",
      "casa\n",
      ".\n",
      "granja\n",
      "nevera\n",
      ".\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f818be96",
   "metadata": {},
   "source": [
    "# Clasificacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías necesarias para leer los archivos de TASS\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer el archivo xml original xml y convertirlo en un diccionario, escogimos el de España\n",
    "with open(\"TASS2019_country_ES_train.xml\") as xml_file: # Lo abro\n",
    "    data_dict = xmltodict.parse(xml_file.read()) # Lo paso a diccionario\n",
    "xml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a json el diccionario\n",
    "json_data = json.dumps(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5a961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir en un archivo el resultado en json\n",
    "with open(\"TASS2019_country_ES_train.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bace8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de datos en la fuente\n",
    "#Original en json\n",
    "fin = open(\"TASS2019_country_ES_train.json\", \"rt\")\n",
    "#Archivo resultante en json\n",
    "fout = open(\"TASS2019_country_ES_train-sintilde.json\", \"wt\")\n",
    "#Procesamiento de lìneas del archivo\n",
    "for line in fin:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n",
    "    fout.write(strtmp1)\n",
    "#cerrar archivos\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tomar los datos del archivo creado en un dataframe\n",
    "train_df = pd.read_json('TASS2019_country_ES_train-sintilde.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e14c7a",
   "metadata": {},
   "source": [
    "#Función para eliminar las menciones a otros usuarios de twitter\n",
    "def filter_reply(content):\n",
    "    temp = content\n",
    "    while temp.find(\"@\") > -1:\n",
    "        print (temp)\n",
    "        temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    \n",
    "print (temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17d29e",
   "metadata": {},
   "source": [
    "#Quitar menciones del texto\n",
    "train_df['content'] = train_df['content'].apply(filter_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f81a9",
   "metadata": {},
   "source": [
    "## Test Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3551c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_resultado = pd.read_csv('final_final_resultado.csv')\n",
    "\n",
    "for line in final_final_resultado:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05df3aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768213567418036224</td>\n",
       "      <td>3429794128</td>\n",
       "      <td>@myendlesshazza a que puto mal escribo b me si...</td>\n",
       "      <td>2016-08-23 22:29:21+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768221670255493120</td>\n",
       "      <td>396616007</td>\n",
       "      <td>quiero mogollon a @albabenito99 pero sobretodo...</td>\n",
       "      <td>2016-08-23 23:01:33+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768221021300264960</td>\n",
       "      <td>2845050061</td>\n",
       "      <td>vale he visto la tia bebiendose su regla y me ...</td>\n",
       "      <td>2016-08-23 22:58:58+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768220253730009088</td>\n",
       "      <td>442100979</td>\n",
       "      <td>@yulian_poe @guillermoterry1 ah mucho mas por ...</td>\n",
       "      <td>2016-08-23 22:55:55+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768231229439311872</td>\n",
       "      <td>529648312</td>\n",
       "      <td>@toni_end seria mejor que dejasen de emitir es...</td>\n",
       "      <td>2016-08-23 23:39:32+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid        user  \\\n",
       "0  768213567418036224  3429794128   \n",
       "1  768221670255493120   396616007   \n",
       "2  768221021300264960  2845050061   \n",
       "3  768220253730009088   442100979   \n",
       "4  768231229439311872   529648312   \n",
       "\n",
       "                                             content  \\\n",
       "0  @myendlesshazza a que puto mal escribo b me si...   \n",
       "1  quiero mogollon a @albabenito99 pero sobretodo...   \n",
       "2  vale he visto la tia bebiendose su regla y me ...   \n",
       "3  @yulian_poe @guillermoterry1 ah mucho mas por ...   \n",
       "4  @toni_end seria mejor que dejasen de emitir es...   \n",
       "\n",
       "                        date lang sentiment  \n",
       "0  2016-08-23 22:29:21+00:00   es         0  \n",
       "1  2016-08-23 23:01:33+00:00   es         1  \n",
       "2  2016-08-23 22:58:58+00:00   es         0  \n",
       "3  2016-08-23 22:55:55+00:00   es         1  \n",
       "4  2016-08-23 23:39:32+00:00   es         0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quitar columnas sin clasificación de sentimiento \n",
    "indexNames = final_final_final_resultado[(final_final_final_resultado['sentiment'] == 'none') | (final_final_final_resultado['sentiment'] == 'neu')].index\n",
    "final_final_final_resultado.drop(indexNames , inplace=True)\n",
    "final_final_final_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc2ba36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías de aprendizaje\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd6ee403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2854\n",
       "1    1896\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar frecuencias de cada categoría\n",
    "final_final_final_resultado['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d1536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid      0\n",
       "user         0\n",
       "content      0\n",
       "date         0\n",
       "lang         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar si hay datos nulos\n",
    "final_final_final_resultado.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c322a5",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b859d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string #Quita todos los simbolos\n",
    "puntua = string.punctuation + \"¡¿\"\n",
    "puntua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f38db165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.strip()\n",
    "        else:\n",
    "            temp = token\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_spacy and token not in puntua:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb9ff2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Te', 'gustar', 'meetup']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"¡Hola cómo estás!. ¿Te gusta el meetup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a532bbcb",
   "metadata": {},
   "source": [
    "# Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cdae01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librería de vectorización\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "044ba220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función de tokenizado y crear el clasificador lineal\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f03d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear los vectores de datos\n",
    "X = final_final_final_resultado['content']\n",
    "y = final_final_final_resultado['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d13a4bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3800,), (950,), (3800,), (950,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear el vector de entrenamiento como una porción de los datos y dejar el resto para pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9c53347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e2e7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evitar que el formato se tome como unknown\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76491e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7f8e47f58d30>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar el clasificador\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "425f65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el vectos de valores predichos a partir del clasificador\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ec8dc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       578\n",
      "           1       0.84      0.78      0.81       372\n",
      "\n",
      "    accuracy                           0.86       950\n",
      "   macro avg       0.85      0.84      0.85       950\n",
      "weighted avg       0.86      0.86      0.85       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ver la precisión obtenida\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bdea28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86% de 'efectividad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8049049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[522,  56],\n",
       "       [ 81, 291]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la matriz de confusión\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639d21f",
   "metadata": {},
   "source": [
    "Me dan 82 falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e849b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir algunas frases de prueba\n",
    "clf.predict([''])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eb719",
   "metadata": {},
   "source": [
    "# Función-Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d6e63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb957085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 authentication chains\n",
    "\n",
    "consumer_key='jQTzrkE7vlZbg2ntJu4LESCZs'\n",
    "consumer_secret='AS4B8YLOWXMcrHjJyZ8stWcm9Cp2qh0rCdIjiWaPBaTTc22tnO'\n",
    "access_key='902474996-b1ltSFx5Y2EdJfi2s63pghsULdjLTF1lkW6oHBvj'\n",
    "access_secret='TrZr5nGmi2Q4RcAJJ3UHbwuWkAqjcNXOTfCHLm1eOzpnn'\n",
    "\n",
    "# authorize twitter, initialize tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5712035",
   "metadata": {},
   "outputs": [],
   "source": [
    "bully = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d6ea7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKKKKKKKKK\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Read-only application cannot POST.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f43516eb5399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OKKKKKKKKK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: Read-only application cannot POST."
     ]
    }
   ],
   "source": [
    "mentions = api.mentions_timeline(tweet_mode= 'extented')\n",
    "    \n",
    "user = []\n",
    "\n",
    "Bully = pd.DataFrame()\n",
    "    \n",
    "for tweet in mentions:\n",
    "        \n",
    "    if clf.predict([tweet.text]) == 0:\n",
    "        print('OKKKKKKKKK')\n",
    "        \n",
    "        api.create_block(tweet.user.screen_name)\n",
    "        \n",
    "        \n",
    "    elif clf.predict([tweet.text]) == 1:\n",
    "        print('BUUUUENO')\n",
    "        \n",
    "\n",
    "    #print(tweet.created_at)\n",
    "    #print(tweet.text)\n",
    "    #print(tweet.source)\n",
    "    #print(tweet.user.name)\n",
    "        \n",
    "    #user.append(tweet.User)\n",
    "        \n",
    "#for i in tweet.user:\n",
    "        \n",
    "        #name = i.name\n",
    "        #screen_name = i.screen_name\n",
    "        #location = i.location if i.location else ''\n",
    "        #url = 'https://twitter.com/' + i.screen_name\n",
    "        #followers_count = i.followers_count\n",
    "        #verified = i.verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkear menciones\n",
    "\n",
    "def check_mentions():\n",
    "    \n",
    "    mentions = api.mentions_timeline(tweet_mode= 'extented')\n",
    "    \n",
    "    user = []\n",
    "    \n",
    "    for tweet in mentions:\n",
    "        \n",
    "        \n",
    "        print(tweet.created_at)\n",
    "        print(tweet.text)\n",
    "        print(tweet.source)\n",
    "        print(tweet.user.name)\n",
    "        \n",
    "        #user.append(tweet.User)\n",
    "        \n",
    "    #for i in tweet.user:\n",
    "        \n",
    "            #name = i.name\n",
    "            #screen_name = i.screen_name\n",
    "            #location = i.location if i.location else ''\n",
    "            #url = 'https://twitter.com/' + i.screen_name\n",
    "            #followers_count = i.followers_count\n",
    "            #verified = i.verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d865705",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d12667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
