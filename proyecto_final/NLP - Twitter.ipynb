{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38bebac",
   "metadata": {},
   "source": [
    "# Modelo NLP - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f40ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "#!pip3 install scikit-learn\n",
    "#!pip3 install -U spacy\n",
    "#!python3 -m spacy download es\n",
    "#!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0864896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e18caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980d9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9639b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b198602",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ed9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hay un gato en la casa. Hay una granja en la nevera. Esta es una tercera frase'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90a03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay\n",
      "un\n",
      "gato\n",
      "en\n",
      "la\n",
      "casa\n",
      ".\n",
      "Hay\n",
      "una\n",
      "granja\n",
      "en\n",
      "la\n",
      "nevera\n",
      ".\n",
      "Esta\n",
      "es\n",
      "una\n",
      "tercera\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc: #ver los tokens/elemento de texto que encuentra la libreria\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a5e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orac = nlp.create_pipe('sentencizer') # 1ª Tuberia/Fase procesamiento. Esto separa la oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0454801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(orac, before='parser') # Se añade al pipe e identifica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1d4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc = nlp (text) #Volvemos a pasar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9b3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay un gato en la casa.\n",
      "Hay una granja en la nevera.\n",
      "Esta es una tercera frase\n"
     ]
    }
   ],
   "source": [
    "for orac in doc.sents: #Ahora, separa en frase\n",
    "    print(orac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b23c2389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lejos', 'adelante', 'qeu', 'tus', 'ningún', 'hablan', 'estoy', 'quién', 'ciertos', 'gran', 'poner', 'atras', 'cualquier', 'segun', 'cosas', 'quizas', 'sigue', 'lugar', 'ademas', 'trabajas', 'demás', 'hace', 'dónde', 'nuestros', 'cuándo', 'incluso', 'toda', 'sé', 'trabajais', 'van', 'agregó', 'podrias', 'bastante', 'ver', 'ni', 'cuáles', 'él', 'indicó', 'podeis', 'pudo', 'ir', 'hacer', 'tan', 'ésas', 'hacemos', 'tiene', 'claro', 'propios', 'durante', 'nosotras', 'sí', 'pesar', 'ellas', 'tendrán', 'considera', 'vuestro', 'día', 'intentamos', 'quiza', 'mucha', 'haya', 'siguiente', 'aquéllas', 'medio', 'aquella', 'están', 'días', 'mucho', 'le', 'sois', 'deprisa', 'nuestras', 'verdad', 'me', 'breve', 'hacerlo', 'nueva', 'cuenta', 'mí', 'así', 'ella', 'pueda', 'existen', 'gueno', 'habían', 'ultimo', 'primera', 'hicieron', 'suya', 'sola', 'aunque', 'uso', 'estais', 'que', 'en', 'según', 'tal', 'manifestó', 'enfrente', 'encima', 'consiguen', 'mios', 'sean', 'allí', 'pueden', 'hacen', 'trata', 'soyos', 'las', 'uno', 'ante', 'momento', 'nuestra', 'todas', 'fueron', 'primero', 'mis', 'aun', 'conseguir', 'pais', 'usamos', 'muchos', 'yo', 'ayer', 'aquél', 'contra', 'vamos', 'también', 'antes', 'dijeron', 'unos', 'esos', 'trabajamos', 'dio', 'conocer', 'dijo', 'pasado', 'asi', 'tiempo', 'conseguimos', 'diferente', 'aquellas', 'decir', 'sino', 'su', 'muchas', 'ninguno', 'emplear', 'cerca', 'luego', 'el', 'poca', 'nuevos', 'he', 'verdadera', 'cual', 'suyas', 'eres', 'éstos', 'trabajo', 'con', 'primer', 'algún', 'fuimos', 'quienes', 'estas', 'hoy', 'no', 'por', 'posible', 'cierta', 'aún', 'informó', 'pronto', 'contigo', 'saber', 'alguna', 'ha', 'general', 'sabeis', 'diferentes', 'existe', 'haces', 'está', 'tengo', 'tu', 'despues', 'tuvo', 'detrás', 'demasiado', 'mediante', 'ya', 'otro', 'sabemos', 'través', 'habla', 'pues', 'poder', 'creo', 'vuestros', 'aqui', 'hecho', 'lleva', 'conmigo', 'además', 'intentar', 'os', 'nuevo', 'qué', 'aproximadamente', 'solos', 'fue', 'esas', 'actualmente', 'ello', 'ex', 'cuanto', 'da', 'te', 'habrá', 'ustedes', 'nadie', 'empleas', 'fuera', 'siendo', 'somos', 'mi', 'quien', 'nuevas', 'entonces', 'saben', 'debido', 'salvo', 'dicen', 'empleais', 'menos', 'podriais', 'usan', 'aseguró', 'tenía', 'delante', 'siete', 'para', 'suyo', 'tenemos', 'ésos', 'última', 'siempre', 'dan', 'este', 'dentro', 'ocho', 'dieron', 'varias', 'horas', 'ahi', 'alguno', 'dicho', 'tras', 'han', 'menudo', 'cuanta', 'aquello', 'cuánto', 'la', 'encuentra', 'otros', 'ellos', 'verdadero', 'soy', 'éste', 'ampleamos', 'acuerdo', 'eramos', 'consideró', 'excepto', 'bien', 'queremos', 'estos', 'varios', 'hasta', 'mientras', 'vosotras', 'ahora', 'mismos', 'pero', 'estamos', 'ninguna', 'vuestra', 'mías', 'otra', 'ser', 'ambos', 'haber', 'había', 'cuales', 'cuánta', 'sería', 'cuantos', 'nos', 'hacia', 'alrededor', 'despacio', 'cuántos', 'podrá', 'estará', 'son', 'embargo', 'parece', 'ningunos', 'mal', 'añadió', 'porque', 'dar', 'junto', 'podriamos', 'quizás', 'dias', 'podrán', 'haceis', 'quiere', 'más', 'hubo', 'final', 'quizá', 'mío', 'mejor', 'fui', 'ese', 'estan', 'dado', 'fin', 'consigue', 'sus', 'lo', 'esta', 'sólo', 'próximo', 'estuvo', 'intenta', 'sea', 'vuestras', 'mio', 'ésa', 'seis', 'cuantas', 'pocos', 'temprano', 'bueno', 'éstas', 'un', 'pocas', 'informo', 'algunas', 'igual', 'de', 'comentó', 'sera', 'debe', 'último', 'llegó', 'tuyos', 'realizó', 'sabe', 'señaló', 'si', 'antaño', 'mias', 'mismo', 'unas', 'últimas', 'antano', 'mía', 'todavia', 'donde', 'dia', 'intentan', 'adrede', 'eran', 'grandes', 'aquélla', 'míos', 'eras', 'sobre', 'veces', 'tenido', 'sabes', 'tanto', 'quedó', 'ésta', 'vaya', 'consigo', 'manera', 'apenas', 'ciertas', 'tenga', 'usas', 'arribaabajo', 'muy', 'podría', 'estaban', 'podria', 'tener', 'usa', 'cuál', 'últimos', 'eso', 'vosotros', 'los', 'respecto', 'segundo', 'tuya', 'raras', 'sin', 'trabaja', 'dice', 'estar', 'ahí', 'afirmó', 'supuesto', 'se', 'tambien', 'anterior', 'tendrá', 'mayor', 'intentas', 'esto', 'podrian', 'bajo', 'nosotros', 'va', 'aquel', 'misma', 'nunca', 'usais', 'hizo', 'todavía', 'como', 'estaba', 'al', 'nuestro', 'parte', 'dos', 'buenos', 'cómo', 'podemos', 'todo', 'otras', 'mia', 'hemos', 'será', 'realizar', 'puedo', 'vais', 'tercera', 'quiénes', 'solo', 'voy', 'tampoco', 'vez', 'cada', 'próximos', 'buena', 'explicó', 'puede', 'realizado', 'cierto', 'estado', 'aquéllos', 'les', 'emplean', 'buen', 'largo', 'enseguida', 'llevar', 'desde', 'mas', 'serán', 'teneis', 'consigues', 'debajo', 'valor', 'aquí', 'casi', 'cuántas', 'una', 'tarde', 'algunos', 'trabajan', 'intentais', 'ningunas', 'habia', 'hay', 'primeros', 'propia', 'nada', 'poco', 'todos', 'esa', 'expresó', 'ejemplo', 'tuyo', 'cuatro', 'proximo', 'buenas', 'aquellos', 'intento', 'partir', 'ti', 'trabajar', 'arriba', 'tres', 'principalmente', 'algo', 'es', 'paìs', 'mencionó', 'modo', 'detras', 'estados', 'tuyas', 'deben', 'pasada', 'propias', 'peor', 'usar', 'lado', 'entre', 'total', 'usted', 'después', 'repente', 'tú', 'haciendo', 'mismas', 'tienen', 'sido', 'solamente', 'cinco', 'segunda', 'hago', 'dejó', 'cuando', 'ése', 'propio', 'alli', 'podrían', 'era', 'empleo', 'del', 'solas']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a eliminar las stop words\n",
    "\n",
    "stopwords_spacy = list(STOP_WORDS) #Checkar cuales son\n",
    "print(stopwords_spacy)\n",
    "len(stopwords_spacy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879801bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meter?{'todo', 'seáis', 'estada', 'estoy', 'se', 'habrás', 'hubierais', 'hubiésemos', 'esta', 'le', 'habidos', 'estuviesen', 'has', 'suyas', 'ha', 'tú', 'tuviese', 'como', 'eres', 'tenga', 'habíais', 'hubiese', 'sentida', 'fuerais', 'es', 'seré', 'mía', 'tengan', 'me', 'nosotros', 'quienes', 'mi', 'otra', 'estarán', 'hubieseis', 'cuando', 'fueran', 'seamos', 'vosotras', 'o', 'tuya', 'eso', 'fuésemos', 'en', 'siente', 'tendrían', 'hayan', 'el', 'tendrás', 'fui', 'tiene', 'habiendo', 'del', 'habríais', 'estar', 'vuestras', 'nuestro', 'serías', 'estos', 'hayáis', 'seremos', 'tenemos', 'ellas', 'tuvierais', 'teníamos', 'mías', 'una', 'nada', 'habré', 'tu', 'sus', 'hube', 'no', 'estuvieron', 'y', 'estabas', 'fueses', 'sean', 'tendríamos', 'vuestra', 'esto', 'habrán', 'tengo', 'poco', 'tenidas', 'estadas', 'tenida', 'estuve', 'otro', 'míos', 'tuyas', 'al', 'otras', 'muchos', 'estarían', 'estéis', 'sí', 'estuvieras', 'te', 'tuvieran', 'estaremos', 'algo', 'nosotras', 'habéis', 'seríais', 'suyo', 'hubiéramos', 'tuviera', 'habrá', 'estaréis', 'de', 'había', 'tendríais', 'habías', 'hubiesen', 'otros', 'hubiste', 'ti', 'sentido', 'fuimos', 'tuyos', 'porque', 'tengáis', 'qué', 'fuese', 'la', 'su', 'nuestras', 'están', 'tuvimos', 'estuviésemos', 'éramos', 'muy', 'mucho', 'vosotros', 'algunas', 'he', 'sentidos', 'estuviese', 'ella', 'fuiste', 'estarás', 'fuéramos', 'uno', 'tenían', 'fueseis', 'ni', 'suyos', 'estaría', 'nuestros', 'estén', 'os', 'tengamos', 'estarías', 'les', 'estábamos', 'también', 'e', 'tendrán', 'está', 'estés', 'tuviésemos', 'sentidas', 'antes', 'mí', 'estemos', 'estaríais', 'soy', 'estuvisteis', 'quien', 'tuvieses', 'estáis', 'que', 'fueron', 'esas', 'hubimos', 'tuvieras', 'entre', 'tuviste', 'unos', 'esté', 'serán', 'habida', 'estaban', 'hayamos', 'fue', 'todos', 'estaríamos', 'serás', 'ellos', 'estado', 'tenías', 'estando', 'estaba', 'estuviéramos', 'vuestros', 'habrías', 'hemos', 'fuisteis', 'tenidos', 'desde', 'esa', 'habría', 'tenéis', 'estuvierais', 'tendré', 'tendrías', 'hayas', 'hubieron', 'sintiendo', 'con', 'habréis', 'habido', 'tendremos', 'estad', 'teniendo', 'vuestro', 'será', 'ya', 'fuera', 'durante', 'hay', 'hubieses', 'han', 'más', 'estuviera', 'estuvieses', 'habíamos', 'son', 'para', 'tenido', 'tuvieron', 'sois', 'esos', 'por', 'estamos', 'sin', 'ese', 'nos', 'tus', 'tuvieseis', 'cual', 'hubisteis', 'habríamos', 'habrían', 'tenía', 'suya', 'eran', 'contra', 'mis', 'habremos', 'estuviste', 'a', 'hubieras', 'estuvieran', 'hubo', 'sentid', 'sobre', 'seas', 'somos', 'algunos', 'pero', 'eras', 'erais', 'tuve', 'hubiera', 'sería', 'hubieran', 'tened', 'tuyo', 'fuesen', 'tendrá', 'tienen', 'seréis', 'mío', 'sea', 'hasta', 'teníais', 'ante', 'tendría', 'nuestra', 'tuviesen', 'habían', 'habidas', 'este', 'un', 'serían', 'donde', 'lo', 'era', 'fueras', 'haya', 'estados', 'seríamos', 'tienes', 'estará', 'estabais', 'estuvieseis', 'estaré', 'las', 'los', 'tuviéramos', 'estás', 'tuvo', 'tendréis', 'tengas', 'estuvo', 'estas', 'él', 'tanto', 'estuvimos', 'tuvisteis', 'yo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c687157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato\n",
      "casa\n",
      ".\n",
      "granja\n",
      "nevera\n",
      ".\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846201c",
   "metadata": {},
   "source": [
    "# Clasificacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89875052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d924708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías necesarias para leer los archivos de TASS\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f7c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer el archivo xml original xml y convertirlo en un diccionario, escogimos el de España\n",
    "with open(\"TASS2019_country_ES_train.xml\") as xml_file: # Lo abro\n",
    "    data_dict = xmltodict.parse(xml_file.read()) # Lo paso a diccionario\n",
    "xml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0cf321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a json el diccionario\n",
    "json_data = json.dumps(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir en un archivo el resultado en json\n",
    "with open(\"TASS2019_country_ES_train.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef71060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de datos en la fuente\n",
    "#Original en json\n",
    "fin = open(\"TASS2019_country_ES_train.json\", \"rt\")\n",
    "#Archivo resultante en json\n",
    "fout = open(\"TASS2019_country_ES_train-sintilde.json\", \"wt\")\n",
    "#Procesamiento de lìneas del archivo\n",
    "for line in fin:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n",
    "    fout.write(strtmp1)\n",
    "#cerrar archivos\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d6bfaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768213567418036224</td>\n",
       "      <td>3429794128</td>\n",
       "      <td>@myendlesshazza a que puto mal escribo b me si...</td>\n",
       "      <td>2016-08-23 22:29:21+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768221670255493120</td>\n",
       "      <td>396616007</td>\n",
       "      <td>quiero mogollon a @albabenito99 pero sobretodo...</td>\n",
       "      <td>2016-08-23 23:01:33+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768221021300264960</td>\n",
       "      <td>2845050061</td>\n",
       "      <td>vale he visto la tia bebiendose su regla y me ...</td>\n",
       "      <td>2016-08-23 22:58:58+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768220253730009088</td>\n",
       "      <td>442100979</td>\n",
       "      <td>@yulian_poe @guillermoterry1 ah mucho mas por ...</td>\n",
       "      <td>2016-08-23 22:55:55+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768231229439311872</td>\n",
       "      <td>529648312</td>\n",
       "      <td>@toni_end seria mejor que dejasen de emitir es...</td>\n",
       "      <td>2016-08-23 23:39:32+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid        user  \\\n",
       "0  768213567418036224  3429794128   \n",
       "1  768221670255493120   396616007   \n",
       "2  768221021300264960  2845050061   \n",
       "3  768220253730009088   442100979   \n",
       "4  768231229439311872   529648312   \n",
       "\n",
       "                                             content  \\\n",
       "0  @myendlesshazza a que puto mal escribo b me si...   \n",
       "1  quiero mogollon a @albabenito99 pero sobretodo...   \n",
       "2  vale he visto la tia bebiendose su regla y me ...   \n",
       "3  @yulian_poe @guillermoterry1 ah mucho mas por ...   \n",
       "4  @toni_end seria mejor que dejasen de emitir es...   \n",
       "\n",
       "                       date lang sentiment  \n",
       "0 2016-08-23 22:29:21+00:00   es         0  \n",
       "1 2016-08-23 23:01:33+00:00   es         1  \n",
       "2 2016-08-23 22:58:58+00:00   es         0  \n",
       "3 2016-08-23 22:55:55+00:00   es         1  \n",
       "4 2016-08-23 23:39:32+00:00   es         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tomar los datos del archivo creado en un dataframe\n",
    "train_df = pd.read_json('TASS2019_country_ES_train-sintilde.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc4775d",
   "metadata": {},
   "source": [
    "#Función para eliminar las menciones a otros usuarios de twitter\n",
    "def filter_reply(content):\n",
    "    temp = content\n",
    "    while temp.find(\"@\") > -1:\n",
    "        print (temp)\n",
    "        temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b9b3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7af25611517b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp' is not defined"
     ]
    }
   ],
   "source": [
    "temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    \n",
    "print (temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc9ffc",
   "metadata": {},
   "source": [
    "#Quitar menciones del texto\n",
    "train_df['content'] = train_df['content'].apply(filter_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a1720",
   "metadata": {},
   "source": [
    "## Test Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "187b2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_resultado = pd.read_csv('final_resultado.csv')\n",
    "\n",
    "for line in final_resultado:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b8ba1a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[<built-in method index of list object at 0x7fee7e153c80>] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-022e8704ca20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Quitar columnas sin clasificación de sentimiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindexNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_resultado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_resultado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'neu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_resultado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexNames\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfinal_resultado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4306\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4307\u001b[0m         \"\"\"\n\u001b[0;32m-> 4308\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4309\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4153\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4186\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4188\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4189\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/clase/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5591\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5592\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[<built-in method index of list object at 0x7fee7e153c80>] not found in axis'"
     ]
    }
   ],
   "source": [
    "#Quitar columnas sin clasificación de sentimiento \n",
    "indexNames = [(final_resultado['sentiment'] == 'none') | (final_resultado['sentiment'] == 'neu')].index\n",
    "final_resultado.drop(indexNames , inplace=True)\n",
    "final_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "214a08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías de aprendizaje\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dc8b80b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1574\n",
       "1    1172\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar frecuencias de cada categoría\n",
    "final['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2521f57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid      0\n",
       "user         0\n",
       "content      0\n",
       "date         0\n",
       "lang         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar si hay datos nulos\n",
    "final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99e3f2",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ba936c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string #Quita todos los simbolos\n",
    "puntua = string.punctuation + \"¡¿\"\n",
    "puntua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35a3ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.strip()\n",
    "        else:\n",
    "            temp = token\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_spacy and token not in puntua:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e033361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Te', 'gustar', 'meetup']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"¡Hola cómo estás!. ¿Te gusta el meetup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f920d",
   "metadata": {},
   "source": [
    "# Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb4d7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librería de vectorización\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "74865fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función de tokenizado y crear el clasificador lineal\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b25f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear los vectores de datos\n",
    "X = final['content']\n",
    "y = final['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6218b3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2196,), (550,), (2196,), (550,))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear el vector de entrenamiento como una porción de los datos y dejar el resto para pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "64778f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1f858e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#evitar que el formato se tome como unknown\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3aabe439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7fee7d5031f0>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar el clasificador\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19b463b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el vectos de valores predichos a partir del clasificador\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "22a4d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.80      0.76       308\n",
      "           1       0.71      0.60      0.65       242\n",
      "\n",
      "    accuracy                           0.71       550\n",
      "   macro avg       0.71      0.70      0.70       550\n",
      "weighted avg       0.71      0.71      0.71       550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ver la precisión obtenida\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d1c63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71% de 'efectividad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "72feadf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[247,  61],\n",
       "       [ 96, 146]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la matriz de confusión\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0098e2",
   "metadata": {},
   "source": [
    "Me dan 42 falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fd1dd55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir algunas frases de prueba\n",
    "clf.predict(['Realmente me gustó mucho este ejercicio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1301b2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una positiva\n",
    "clf.predict(['Pablo Iglesias es el mejor del mundo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6923c2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una negativa\n",
    "clf.predict(['Pablo Iglesias es un hijo de puta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8f4d9c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una negativa\n",
    "clf.predict(['persona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c09d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87149393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
