{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38bebac",
   "metadata": {},
   "source": [
    "# Modelo NLP - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f40ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "#!pip3 install scikit-learn\n",
    "#!pip3 install -U spacy\n",
    "#!python3 -m spacy download es\n",
    "#!python3 -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb55a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9292303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980d9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "738582ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b198602",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ed9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hay un gato en la casa. Hay una granja en la nevera. Esta es una tercera frase'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90a03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay\n",
      "un\n",
      "gato\n",
      "en\n",
      "la\n",
      "casa\n",
      ".\n",
      "Hay\n",
      "una\n",
      "granja\n",
      "en\n",
      "la\n",
      "nevera\n",
      ".\n",
      "Esta\n",
      "es\n",
      "una\n",
      "tercera\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc: #ver los tokens/elemento de texto que encuentra la libreria\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a5e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orac = nlp.create_pipe('sentencizer') # 1ª Tuberia/Fase procesamiento. Esto separa la oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0454801",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(orac, before='parser') # Se añade al pipe e identifica "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab1d4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc = nlp (text) #Volvemos a pasar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d41f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay un gato en la casa.\n",
      "Hay una granja en la nevera.\n",
      "Esta es una tercera frase\n"
     ]
    }
   ],
   "source": [
    "for orac in doc.sents: #Ahora, separa en frase\n",
    "    print(orac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f2045bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tres', 'tuvo', 'sino', 'supuesto', 'habrá', 'podrian', 'encuentra', 'quedó', 'fui', 'dicho', 'sí', 'sabe', 'podrá', 'hasta', 'respecto', 'mía', 'adelante', 'tendrá', 'aunque', 'siempre', 'tercera', 'bastante', 'peor', 'trabajas', 'debajo', 'parte', 'través', 'ti', 'vuestras', 'poco', 'usar', 'horas', 'nada', 'soy', 'cuánto', 'conseguir', 'medio', 'demás', 'ninguna', 'allí', 'consigues', 'ésa', 'aun', 'saben', 'consideró', 'añadió', 'su', 'fuera', 'conocer', 'serán', 'veces', 'mismos', 'última', 'modo', 'pesar', 'segunda', 'ésta', 'repente', 'estuvo', 'las', 'ir', 'ver', 'intentas', 'éste', 'podriamos', 'parece', 'qeu', 'aquél', 'intentan', 'en', 'dónde', 'proximo', 'siete', 'mí', 'emplear', 'podeis', 'al', 'intento', 'primeros', 'fin', 'ningún', 'decir', 'mías', 'podría', 'han', 'teneis', 'no', 'lejos', 'excepto', 'tengo', 'todavía', 'alli', 'enseguida', 'cuál', 'hecho', 'llegó', 'míos', 'cual', 'ellos', 'incluso', 'día', 'éstos', 'junto', 'alguno', 'sigue', 'cuántas', 'antes', 'mientras', 'habla', 'una', 'buenos', 'cuatro', 'existe', 'realizó', 'suyas', 'alguna', 'éstas', 'usais', 'pasada', 'va', 'quiere', 'salvo', 'esos', 'ustedes', 'aquellas', 'delante', 'somos', 'trabajais', 'igual', 'hay', 'estais', 'deprisa', 'tenga', 'nosotras', 'todos', 'ella', 'cuenta', 'general', 'ahora', 'momento', 'saber', 'alrededor', 'entre', 'voy', 'podrían', 'además', 'indicó', 'dias', 'sabeis', 'todavia', 'sido', 'haber', 'intentar', 'fueron', 'apenas', 'breve', 'pero', 'largo', 'van', 'había', 'esta', 'mayor', 'estos', 'anterior', 'llevar', 'sabes', 'la', 'cuando', 'adrede', 'cinco', 'segun', 'hacen', 'trabajar', 'del', 'pronto', 'emplean', 'por', 'esto', 'días', 'vuestra', 'solas', 'esas', 'intenta', 'cómo', 'soyos', 'nuestra', 'el', 'algunos', 'mucho', 'da', 'tarde', 'tal', 'primera', 'gran', 'embargo', 'mis', 'consigue', 'últimos', 'para', 'esa', 'cuándo', 'menos', 'misma', 'segundo', 'poner', 'aquello', 'cerca', 'cuantas', 'mios', 'vaya', 'ciertos', 'tampoco', 'ya', 'arriba', 'creo', 'nuestras', 'casi', 'estaban', 'hacemos', 'unas', 'hace', 'pues', 'muy', 'quién', 'haciendo', 'mias', 'mismo', 'menudo', 'fuimos', 'me', 'verdadero', 'dice', 'contigo', 'cuánta', 'realizado', 'mío', 'sola', 'próximos', 'señaló', 'entonces', 'estamos', 'próximo', 'último', 'habia', 'poder', 'son', 'sin', 'cualquier', 'cosas', 'lado', 'nos', 'deben', 'final', 'mia', 'temprano', 'otros', 'buena', 'dan', 'ahi', 'es', 'nuevas', 'aquella', 'sé', 'trabaja', 'vamos', 'solos', 'buenas', 'siendo', 'estará', 'intentais', 'pueden', 'ser', 'dentro', 'consigo', 'tras', 'ello', 'sería', 'algún', 'sobre', 'actualmente', 'cuales', 'mediante', 'conmigo', 'estados', 'quizas', 'hemos', 'solo', 'aún', 'ambos', 'buen', 'pasado', 'cada', 'comentó', 'conseguimos', 'nuevos', 'vuestro', 'hoy', 'mas', 'vuestros', 'expresó', 'seis', 'lleva', 'arribaabajo', 'quiénes', 'tú', 'intentamos', 'de', 'dieron', 'dos', 'eran', 'suyo', 'tendrán', 'últimas', 'quien', 'vez', 'él', 'grandes', 'estado', 'propios', 'tener', 'nuestros', 'algo', 'contra', 'ejemplo', 'ése', 'mal', 'este', 'hacerlo', 'nunca', 'ningunos', 'tambien', 'demasiado', 'tiempo', 'dijo', 'usted', 'le', 'será', 'sabemos', 'tiene', 'yo', 'estaba', 'haces', 'detras', 'varias', 'diferentes', 'vosotros', 'más', 'según', 'sera', 'suya', 'ahí', 'ninguno', 'ex', 'raras', 'nueva', 'ciertas', 'antaño', 'puedo', 'que', 'ese', 'uso', 'vais', 'haceis', 'nuestro', 'eres', 'realizar', 'debido', 'otra', 'quizá', 'tuyos', 'estan', 'usan', 'ellas', 'desde', 'manifestó', 'nadie', 'encima', 'bajo', 'tuyo', 'despacio', 'podrán', 'debe', 'aquéllas', 'ésas', 'siguiente', 'ha', 'dia', 'ademas', 'podria', 'consiguen', 'te', 'usamos', 'trabajan', 'donde', 'dijeron', 'afirmó', 'acuerdo', 'muchos', 'informo', 'cuáles', 'sea', 'estoy', 'existen', 'claro', 'dado', 'hubo', 'enfrente', 'quizás', 'aproximadamente', 'dio', 'aquí', 'antano', 'lugar', 'pais', 'os', 'algunas', 'luego', 'aquéllos', 'queremos', 'si', 'otras', 'cuanto', 'cuántos', 'pocos', 'mucha', 'atras', 'tenido', 'podriais', 'sois', 'considera', 'bueno', 'sean', 'los', 'lo', 'usas', 'aqui', 'bien', 'un', 'hicieron', 'toda', 'verdadera', 'se', 'ningunas', 'cierto', 'cuanta', 'tenemos', 'usa', 'varios', 'como', 'posible', 'unos', 'porque', 'partir', 'aquel', 'total', 'empleas', 'qué', 'quienes', 'hacia', 'agregó', 'ayer', 'estar', 'paìs', 'diferente', 'está', 'tu', 'dar', 'eso', 'durante', 'mejor', 'tienen', 'solamente', 'hago', 'todas', 'propia', 'manera', 'podemos', 'aquellos', 'mi', 'dicen', 'empleais', 'poca', 'despues', 'detrás', 'eramos', 'hizo', 'podrias', 'asi', 'fue', 'así', 'les', 'vosotras', 'uno', 'nosotros', 'ampleamos', 'gueno', 'pueda', 'tanto', 'trabajo', 'haya', 'primer', 'propio', 'trata', 'ante', 'cuantos', 'hacer', 'después', 'sus', 'tuyas', 'propias', 'he', 'puede', 'eras', 'habían', 'empleo', 'primero', 'mencionó', 'sólo', 'quiza', 'verdad', 'ni', 'tus', 'todo', 'con', 'cierta', 'ocho', 'principalmente', 'trabajamos', 'informó', 'hablan', 'muchas', 'ésos', 'era', 'otro', 'aquélla', 'tan', 'estas', 'mismas', 'están', 'tenía', 'también', 'aseguró', 'mio', 'nuevo', 'ultimo', 'dejó', 'pocas', 'explicó', 'tuya', 'pudo', 'valor']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a eliminar las stop words\n",
    "\n",
    "stopwords_spacy = list(STOP_WORDS) #Checkar cuales son\n",
    "print(stopwords_spacy)\n",
    "len(stopwords_spacy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b521de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meter?{'todo', 'seáis', 'estada', 'estoy', 'se', 'habrás', 'hubierais', 'hubiésemos', 'esta', 'le', 'habidos', 'estuviesen', 'has', 'suyas', 'ha', 'tú', 'tuviese', 'como', 'eres', 'tenga', 'habíais', 'hubiese', 'sentida', 'fuerais', 'es', 'seré', 'mía', 'tengan', 'me', 'nosotros', 'quienes', 'mi', 'otra', 'estarán', 'hubieseis', 'cuando', 'fueran', 'seamos', 'vosotras', 'o', 'tuya', 'eso', 'fuésemos', 'en', 'siente', 'tendrían', 'hayan', 'el', 'tendrás', 'fui', 'tiene', 'habiendo', 'del', 'habríais', 'estar', 'vuestras', 'nuestro', 'serías', 'estos', 'hayáis', 'seremos', 'tenemos', 'ellas', 'tuvierais', 'teníamos', 'mías', 'una', 'nada', 'habré', 'tu', 'sus', 'hube', 'no', 'estuvieron', 'y', 'estabas', 'fueses', 'sean', 'tendríamos', 'vuestra', 'esto', 'habrán', 'tengo', 'poco', 'tenidas', 'estadas', 'tenida', 'estuve', 'otro', 'míos', 'tuyas', 'al', 'otras', 'muchos', 'estarían', 'estéis', 'sí', 'estuvieras', 'te', 'tuvieran', 'estaremos', 'algo', 'nosotras', 'habéis', 'seríais', 'suyo', 'hubiéramos', 'tuviera', 'habrá', 'estaréis', 'de', 'había', 'tendríais', 'habías', 'hubiesen', 'otros', 'hubiste', 'ti', 'sentido', 'fuimos', 'tuyos', 'porque', 'tengáis', 'qué', 'fuese', 'la', 'su', 'nuestras', 'están', 'tuvimos', 'estuviésemos', 'éramos', 'muy', 'mucho', 'vosotros', 'algunas', 'he', 'sentidos', 'estuviese', 'ella', 'fuiste', 'estarás', 'fuéramos', 'uno', 'tenían', 'fueseis', 'ni', 'suyos', 'estaría', 'nuestros', 'estén', 'os', 'tengamos', 'estarías', 'les', 'estábamos', 'también', 'e', 'tendrán', 'está', 'estés', 'tuviésemos', 'sentidas', 'antes', 'mí', 'estemos', 'estaríais', 'soy', 'estuvisteis', 'quien', 'tuvieses', 'estáis', 'que', 'fueron', 'esas', 'hubimos', 'tuvieras', 'entre', 'tuviste', 'unos', 'esté', 'serán', 'habida', 'estaban', 'hayamos', 'fue', 'todos', 'estaríamos', 'serás', 'ellos', 'estado', 'tenías', 'estando', 'estaba', 'estuviéramos', 'vuestros', 'habrías', 'hemos', 'fuisteis', 'tenidos', 'desde', 'esa', 'habría', 'tenéis', 'estuvierais', 'tendré', 'tendrías', 'hayas', 'hubieron', 'sintiendo', 'con', 'habréis', 'habido', 'tendremos', 'estad', 'teniendo', 'vuestro', 'será', 'ya', 'fuera', 'durante', 'hay', 'hubieses', 'han', 'más', 'estuviera', 'estuvieses', 'habíamos', 'son', 'para', 'tenido', 'tuvieron', 'sois', 'esos', 'por', 'estamos', 'sin', 'ese', 'nos', 'tus', 'tuvieseis', 'cual', 'hubisteis', 'habríamos', 'habrían', 'tenía', 'suya', 'eran', 'contra', 'mis', 'habremos', 'estuviste', 'a', 'hubieras', 'estuvieran', 'hubo', 'sentid', 'sobre', 'seas', 'somos', 'algunos', 'pero', 'eras', 'erais', 'tuve', 'hubiera', 'sería', 'hubieran', 'tened', 'tuyo', 'fuesen', 'tendrá', 'tienen', 'seréis', 'mío', 'sea', 'hasta', 'teníais', 'ante', 'tendría', 'nuestra', 'tuviesen', 'habían', 'habidas', 'este', 'un', 'serían', 'donde', 'lo', 'era', 'fueras', 'haya', 'estados', 'seríamos', 'tienes', 'estará', 'estabais', 'estuvieseis', 'estaré', 'las', 'los', 'tuviéramos', 'estás', 'tuvo', 'tendréis', 'tengas', 'estuvo', 'estas', 'él', 'tanto', 'estuvimos', 'tuvisteis', 'yo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625d8bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gato\n",
      "casa\n",
      ".\n",
      "granja\n",
      "nevera\n",
      ".\n",
      "frase\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed04cd",
   "metadata": {},
   "source": [
    "# Clasificacion de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a41342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15cee9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías necesarias para leer los archivos de TASS\n",
    "import xmltodict\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traer el archivo xml original xml y convertirlo en un diccionario, escogimos el de España\n",
    "with open(\"TASS2019_country_ES_train.xml\") as xml_file: # Lo abro\n",
    "    data_dict = xmltodict.parse(xml_file.read()) # Lo paso a diccionario\n",
    "xml_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf348c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a json el diccionario\n",
    "json_data = json.dumps(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir en un archivo el resultado en json\n",
    "with open(\"TASS2019_country_ES_train.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de datos en la fuente\n",
    "#Original en json\n",
    "fin = open(\"TASS2019_country_ES_train.json\", \"rt\")\n",
    "#Archivo resultante en json\n",
    "fout = open(\"TASS2019_country_ES_train-sintilde.json\", \"wt\")\n",
    "#Procesamiento de lìneas del archivo\n",
    "for line in fin:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n",
    "    fout.write(strtmp1)\n",
    "#cerrar archivos\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tomar los datos del archivo creado en un dataframe\n",
    "train_df = pd.read_json('TASS2019_country_ES_train-sintilde.json')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2390950",
   "metadata": {},
   "source": [
    "#Función para eliminar las menciones a otros usuarios de twitter\n",
    "def filter_reply(content):\n",
    "    temp = content\n",
    "    while temp.find(\"@\") > -1:\n",
    "        print (temp)\n",
    "        temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[:temp.find(\"@\")] + temp[(temp.find(\" \",temp.find(\"@\"))):]\n",
    "    \n",
    "print (temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ab74e",
   "metadata": {},
   "source": [
    "#Quitar menciones del texto\n",
    "train_df['content'] = train_df['content'].apply(filter_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b9544",
   "metadata": {},
   "source": [
    "## Test Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b22d6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_final_resultado = pd.read_csv('final_final_resultado.csv')\n",
    "\n",
    "for line in final_final_resultado:\n",
    "\t#Reemplazar los caracteres unicode, no se dejaron tildes porque causan error\n",
    "    strtmp1 = line.replace('\\\\u00f1', 'ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e1', 'a')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00e9', 'e')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00ed', 'i')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00f3', 'o')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fa', 'u')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00bf', '¿')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00a1', '¡')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d1', 'Ñ')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c1', 'A')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00c9', 'E')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00cd', 'I')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00d3', 'O')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00da', 'U')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00fc', 'ü')\n",
    "    strtmp1 = strtmp1.replace('\\\\u00b0', '')\n",
    "    #Quitar el inicio y el fin del json para dejar solo los tweets\n",
    "    strtmp1 = strtmp1.replace('{\"tweets\": {\"tweet\": ', '')\n",
    "    strtmp1 = strtmp1.replace(']}}', ']')\n",
    "    #Quitar el diccionario que contiene la polaridad y dejarla solo con su valor de sentimiento\n",
    "    strtmp1 = strtmp1.replace('\"sentiment\": {\"polarity\": {\"value\": ', '\"sentiment\": ')\n",
    "    strtmp1 = strtmp1.replace('\"NONE\"}}', '\"NONE\"')\n",
    "    #Asignamos al sentimiento positivo el valor de 1\n",
    "    strtmp1 = strtmp1.replace('\"P\"}}', '1')\n",
    "    strtmp1 = strtmp1.replace('\"NEU\"}}', '\"NEU\"')\n",
    "    #Asignamos al sentimiento negativo el valor de 0\n",
    "    strtmp1 = strtmp1.replace('\"N\"}}', '0')\n",
    "    #eliminación de puntuaciones\n",
    "    strtmp1 = re.sub('[¡!#$).;¿?&°]', '', strtmp1.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edfa2bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768213567418036224</td>\n",
       "      <td>3429794128</td>\n",
       "      <td>@myendlesshazza a que puto mal escribo b me si...</td>\n",
       "      <td>2016-08-23 22:29:21+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768221670255493120</td>\n",
       "      <td>396616007</td>\n",
       "      <td>quiero mogollon a @albabenito99 pero sobretodo...</td>\n",
       "      <td>2016-08-23 23:01:33+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768221021300264960</td>\n",
       "      <td>2845050061</td>\n",
       "      <td>vale he visto la tia bebiendose su regla y me ...</td>\n",
       "      <td>2016-08-23 22:58:58+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>768220253730009088</td>\n",
       "      <td>442100979</td>\n",
       "      <td>@yulian_poe @guillermoterry1 ah mucho mas por ...</td>\n",
       "      <td>2016-08-23 22:55:55+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>768231229439311872</td>\n",
       "      <td>529648312</td>\n",
       "      <td>@toni_end seria mejor que dejasen de emitir es...</td>\n",
       "      <td>2016-08-23 23:39:32+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid        user  \\\n",
       "0  768213567418036224  3429794128   \n",
       "1  768221670255493120   396616007   \n",
       "2  768221021300264960  2845050061   \n",
       "3  768220253730009088   442100979   \n",
       "4  768231229439311872   529648312   \n",
       "\n",
       "                                             content  \\\n",
       "0  @myendlesshazza a que puto mal escribo b me si...   \n",
       "1  quiero mogollon a @albabenito99 pero sobretodo...   \n",
       "2  vale he visto la tia bebiendose su regla y me ...   \n",
       "3  @yulian_poe @guillermoterry1 ah mucho mas por ...   \n",
       "4  @toni_end seria mejor que dejasen de emitir es...   \n",
       "\n",
       "                        date lang sentiment  \n",
       "0  2016-08-23 22:29:21+00:00   es         0  \n",
       "1  2016-08-23 23:01:33+00:00   es         1  \n",
       "2  2016-08-23 22:58:58+00:00   es         0  \n",
       "3  2016-08-23 22:55:55+00:00   es         1  \n",
       "4  2016-08-23 23:39:32+00:00   es         0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quitar columnas sin clasificación de sentimiento \n",
    "indexNames = final_final_resultado[(final_final_resultado['sentiment'] == 'none') | (final_final_resultado['sentiment'] == 'neu')].index\n",
    "final_final_resultado.drop(indexNames , inplace=True)\n",
    "final_final_resultado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9808775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías de aprendizaje\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37785c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2854\n",
       "1    1896\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar frecuencias de cada categoría\n",
    "final_final_resultado['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c98ce73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid      0\n",
       "user         0\n",
       "content      0\n",
       "date         0\n",
       "lang         0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificar si hay datos nulos\n",
    "final_final_resultado.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221ac5d",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e53d0572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¡¿'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string #Quita todos los simbolos\n",
    "puntua = string.punctuation + \"¡¿\"\n",
    "puntua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6427c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para limpieza de datos\n",
    "def text_data_cleaning(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != \"-PRON-\":\n",
    "            temp = token.lemma_.strip()\n",
    "        else:\n",
    "            temp = token\n",
    "        tokens.append(temp)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_spacy and token not in puntua:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1c3e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'Te', 'gustar', 'meetup']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_cleaning(\"¡Hola cómo estás!. ¿Te gusta el meetup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f30fea",
   "metadata": {},
   "source": [
    "# Vectorization Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6018493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar librería de vectorización\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71ecd6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir la función de tokenizado y crear el clasificador lineal\n",
    "tfidf = TfidfVectorizer(tokenizer = text_data_cleaning)\n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaf66cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear los vectores de datos\n",
    "X = final_final_resultado['content']\n",
    "y = final_final_resultado['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29b66f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3800,), (950,), (3800,), (950,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear el vector de entrenamiento como una porción de los datos y dejar el resto para pruebas\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bbe05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear un pipeline\n",
    "clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1a05877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#evitar que el formato se tome como unknown\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45bf1d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(tokenizer=<function text_data_cleaning at 0x7fd437b8e310>)),\n",
       "                ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entrenar el clasificador\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9912179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear el vectos de valores predichos a partir del clasificador\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c9f1567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88       578\n",
      "           1       0.84      0.78      0.81       372\n",
      "\n",
      "    accuracy                           0.86       950\n",
      "   macro avg       0.85      0.84      0.85       950\n",
      "weighted avg       0.86      0.86      0.85       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ver la precisión obtenida\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84e1943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86% de 'efectividad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b6b2145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[522,  56],\n",
       "       [ 81, 291]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Crear la matriz de confusión\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5c7ee",
   "metadata": {},
   "source": [
    "Me dan 42 falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86e3227a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predecir algunas frases de prueba\n",
    "clf.predict(['Realmente me gustó mucho este ejercicio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "773aeab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una positiva\n",
    "clf.predict(['Pablo Iglesias es el mejor del mundo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "132f4ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una negativa\n",
    "clf.predict(['Pablo Iglesias es un hijo de puta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a6604d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Una negativa\n",
    "clf.predict(['Eres una rata inmunda, no te mereces ser presidente'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4c5d408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['@PabloIglesias eres bueno'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad0717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
